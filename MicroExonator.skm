#version 0.9.0

import yaml
from collections import defaultdict
import csv

configfile : "config.yaml"
DATA = set([])

def str2bool(v):
  if v==True:
    return True
  elif v==False:
    return False
  else:
    return v.lower() in ("yes", "true", "t", "1")

rule quant:
    input:
        "Report/out.high_quality.txt",
        "Report/out_filtered_ME.PSI.txt",        
        #"Report/stats/Microexons.not_consensus",
        #"Report/stats/Microexons.annotation.stats"        
        
        #"Report/out_filtered_ME.txt"
        #expand("Genome_aligments/{Software}/TOTAL.exons.{Software}", Software=["Hisat2", "STAR", "Olego"])
        # expand("Genome_aligments/{Software}/{sample}.sam.SJ_count", sample=DATA, Software=["Hisat2", "STAR"]),
        #expand("Whippet/Quant/{sample}.psi.gz", sample=DATA),
        #expand("Ground_Truth/{sample}.GT.SJ_count", sample=DATA)

rule discovery:
    input:
        "Round2/ME_canonical_SJ_tags.de_novo.fa"


if 'cluster_metadata' in config:

    cluster_files = defaultdict(list)
    cluster_files_metadata = defaultdict(list)
    single_cell_files = set([])

    with open(config["cluster_metadata"]) as Single_Cell:

        Single_Cell_clustering = csv.DictReader(Single_Cell, delimiter="\t")

        for row in Single_Cell_clustering:

            cluster_files[row[config["cluster_name"]].replace(" ", "_")].append(row[config["file_basename"]])
            single_cell_files.add(row[config["file_basename"]])


#### MicroExonator ####

if ("deletion_penalty" in config)==False:
    config["deletion_penalty"]="6"
    
if ("insertion_penalty" in config)==False:
    config["insertion_penalty"]="2"

config["indel_penalty"] = ",".join([str(config["deletion_penalty"]), str(config["insertion_penalty"])])

if ("ME_DB" in config)==False:
    config["ME_DB"]="touch/VastDb.bed12"

if ("paired_samples" in config)==False:
    config["paired_samples"]="F"

if ("min_reads_PSI" in config)==False:
    config["min_reads_PSI"]="5"


include : "rules/init.skm"
include : "rules/Get_data.skm"


rule bamfiles:
    input:
        expand("Whippet/BAM/{samples}.bam", samples=DATA), 
        expand("Whippet/BAM/{samples}.bam.bai", samples=DATA)


if ("downstream_only" in config)==False:

    include : "rules/Round1.skm"
    include : "rules/Round1_post_processing.skm"
    include : "rules/Round2.skm"
    include : "rules/Round2_post_processing.skm"

elif str2bool(config["downstream_only"])==False:

    include : "rules/Round1.skm"
    include : "rules/Round1_post_processing.skm"
    include : "rules/Round2.skm"
    include : "rules/Round2_post_processing.skm"

else:
    pass

##### Downstream Analysis ####

if "whippet_bin_folder" in config:
   include : "rules/Whippet_quant.skm"

if "whippet_delta" in config:
   with open(config["whippet_delta"], 'r') as stream:
      whippet_delta = yaml.safe_load(stream)
   include : "rules/Whippet_delta.skm"

#### Single Cell ###

if not "Single_Cell" in config:
   config["Single_Cell"]="F"

if str2bool(config["Single_Cell"]):
   include : "rules/Snakepool.py"   

#### Benchmark ####

#include : "rules/Benchmark.skm"





#### Re-run incomplete round1 ####

import os

round1_incomplete = []

for file in DATA:
    if os.path.isfile('./Round1/' + file  + '.sam.row_ME.filter1')!=True:
        round1_incomplete.append(file)
  
rule rerun_incomplete_round1:
    input:
        expand("Round1/{sample}.sam.row_ME.filter1", sample=round1_incomplete )
        
        
        
round2_incomplete = []

for file in DATA:
    if os.path.isfile('./Round2/' + file  + '.sam.pre_processed.filter1.ME_SJ_coverage')!=True:
        round2_incomplete.append(file)
  
rule rerun_incomplete_round2:
    input:
        expand("Round2/{sample}.sam.pre_processed.filter1.ME_SJ_coverage", sample=round2_incomplete )
